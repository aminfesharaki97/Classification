---
title: "A"
author: "Amin"
date: "8/8/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r global.options, include = FALSE}
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```


**Use agglomerative clustering and divisive clustering on this dataset to find out which players have similar performance in the same season. Visualize the clusters using dendrograms for both types of clustering models**

```{r}
library(cluster) #Finding Groups in Data
library(factoextra) #Determine optimal number clusters for a given clustering method
library(NbClust) #computing about 30 methods at once in order to find optimal number of clusters
library(ggplot2) #Data Visualization with ggplot2
```



```{r}
setwd("/Users/datascience/Desktop/Data Science/R/R Files")
lpga<- read.csv("Lpga_2008_Dataset.csv", header = TRUE, sep = ',')
```


```{r}
head(lpga)
```
*For this clustering exercise, you are going to use the data on women professional golfers’ performance on the LPGA, 2008 tour (“lpga2008.csv” dataset). The dataset has the following attributes:*
*i. Golfer: name of the player*
*ii. Average Drive distance*
*iii. Fairway Percentage*
*iv. Greens in regulation: in percentage*
*v. Average putts per round*
*vi. Sand attempts per round*
*vii. Sand saves: in percentage*
*viii. Total Winnings per round*
*ix. Log: Calculated as (Total Win/Round)*
*x. Total Rounds*
*xi. Id: Unique ID representing each player*

```{r}
#Gather basic useful information about the dataset

#Summary of Data Set
summary(lpga)

#Check to see if there is any missing values
any(is.na(lpga))
```

```{r}
#Drop the ID row (ID does not determine anything based on player performance)
df <- subset(lpga, select = -c(Id) )
```

```{r}
dnew <-subset(lpga, select = -c(TotalWinnings,Id))
```


```{r}

dn <- subset(lpga, select = -c(Golfer) )
```

```{r}
#remove Names and ID to determine the # of clusters 
NumClus <- subset(lpga, select = -c(Golfer, Id) ) 
```

```{r}
#Elbow Method for Number of
fviz_nbclust(NumClus, hcut, method = "wss")
```
*The Elbow Method Indicates 4 is the Optimal Number of Clusters*
```{r}
#Silhouette Method for Number of Clusters
fviz_nbclust(NumClus, hcut, method = "silhouette")
```
*The Silhouette Method Indicates 6 as the optimal number of clusters*
```{r}
#Gap Statistic for # of Clusters
gap_stat <- clusGap(NumClus, FUN=hcut , nstart = 25, K.max=10, B=50)
#Plot number of clusters vs gap statistic
fviz_gap_stat(gap_stat)
```
*The gap statistical method shows that 1 is the best, but in reality we should not have 1 cluster for this data set. Other possible values would be 4-5 clusters and 9 clusters. This graph should not be used when determining optimal number of clusters*


```{r}  
#30 indices for choosing the best number of clusters
nb <- NbClust(NumClus, distance = "euclidean", min.nc = 2,
        max.nc = 10, method = "complete")
```

```{r}
#Result of NbClust function
fviz_nbclust(nb)
```
*Among all indices, the best number of clusters is 5*

```{r}
res.agnes <- agnes (x = df, #Data matrix
                    stand = TRUE, #Standardize the Data
                    metric = "euclidean", #metric for distance matrix
                    method = "ward" #linkage method
                    )


res.diana <- diana (x = df, # Data matrix
                    stand = TRUE, #Standardize the Data
                    metric = "euclidean" #metric for distance matrix
                    )
```
*Ward's (minimum variance) method minimizes the total within-cluster variance. Each step the pair of clusters with minimum between-cluster distance are merged together. Additionally Ward is the most effective method for noisy data*

*The Euclidean distance is chosen so that the observations with high values of features will be clustered together as well as low values of features*

```{r}
res.agnes
```

```{r}
fviz_dend(res.agnes, cex=0.5, k=5)
```
```{r}

```


```{r}
fviz_dend(res.diana, cex=0.5, k=5, labels=df)
```


```{r}
distance <- dist(as.matrix(lpga))
```
```{r}
hc <- hclust(distance)
```

```{r}
plot(hc)
```





```{r}
distan <- dist(as.matrix(dn))
```

```{r}
hcc <- hclust(distan)
```

```{r}
plot(hcc)
```

```{r}
res.agnesN <- agnes (x = dnew, #Data matrix
                    stand = TRUE, #Standardize the Data
                    metric = "euclidean", #metric for distance matrix
                    method = "ward" #linkage method
                    )


res.dianaN <- diana (x = dnew, # Data matrix
                    stand = TRUE, #Standardize the Data
                    metric = "euclidean" #metric for distance matrix
                    )
```

```{r}
plot(res)
```


```{r}
fviz_dend(res.agnesN, cex=0.5, k=5)
```
```{r}
distance <-dist(as.matrix(df))
```
```{r}
hc <- hclust(distance)
plot(hc, labels = df$Golfer, hang =-1)
```


